{"cells":[{"cell_type":"markdown","metadata":{"id":"GpY-ArzB1rH-"},"source":["# **Disease Detection using Symptoms and Treatment recommendation**"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_VLPHFN2Edo","executionInfo":{"status":"ok","timestamp":1668954253316,"user_tz":-420,"elapsed":21294,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}},"outputId":"19733f8f-3740-46b3-e14c-112fd1ef5e21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"etxuEgYCG7bC","outputId":"85b64d13-ddd6-4b31-c6e9-3bfc1272db3e","executionInfo":{"status":"ok","timestamp":1668954305147,"user_tz":-420,"elapsed":51834,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["# importing nltk to download resources for stopwords and wordnet\n","import nltk\n","nltk.download('all')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"code","source":["pip install -U deep-translator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kE5R4GOaj2ng","executionInfo":{"status":"ok","timestamp":1668954311232,"user_tz":-420,"elapsed":6096,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}},"outputId":"7f56a68e-6e8c-4fff-e608-b294d2aa1cdf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting deep-translator\n","  Downloading deep_translator-1.9.1-py3-none-any.whl (30 kB)\n","Collecting beautifulsoup4<5.0.0,>=4.9.1\n","  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n","\u001b[K     |████████████████████████████████| 128 kB 8.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n","Collecting soupsieve>1.2\n","  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n","Installing collected packages: soupsieve, beautifulsoup4, deep-translator\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed beautifulsoup4-4.11.1 deep-translator-1.9.1 soupsieve-2.3.2.post1\n"]}]},{"cell_type":"code","source":["pip install PySastrawi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXvUX1c7bC1n","executionInfo":{"status":"ok","timestamp":1668954315766,"user_tz":-420,"elapsed":4538,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}},"outputId":"6d68e5c0-0769-428e-baa3-a8964d75ddc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting PySastrawi\n","  Downloading PySastrawi-1.2.0-py2.py3-none-any.whl (210 kB)\n","\u001b[K     |████████████████████████████████| 210 kB 4.7 MB/s \n","\u001b[?25hInstalling collected packages: PySastrawi\n","Successfully installed PySastrawi-1.2.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"id":"CrLG2ksh5w4Z","outputId":"edd9a2ea-0070-4ad2-ffb0-ed90ae9b1f90","executionInfo":{"status":"ok","timestamp":1668954396081,"user_tz":-420,"elapsed":80319,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-3d49dc64-db72-42ea-9e15-d1ecea0a77d5\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3d49dc64-db72-42ea-9e15-d1ecea0a77d5\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Treatment.py to Treatment.py\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-9dd08e49-85cf-4b84-a67e-6623db4b45f1\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-9dd08e49-85cf-4b84-a67e-6623db4b45f1\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving dict.json to dict.json\n"]}],"source":["# importing all libraries\n","import numpy as np\n","import pandas as pd\n","import math\n","from sklearn.model_selection import train_test_split,cross_val_score\n","import math\n","import operator\n","import pickle\n","import re\n","#from nltk.stem import WordNetLemmatizer\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import RegexpTokenizer\n","from statistics import mean\n","from nltk.corpus import wordnet \n","import requests\n","from bs4 import BeautifulSoup\n","from itertools import combinations\n","from time import time\n","from collections import Counter\n","import operator\n","import warnings\n","import json\n","from google.colab import files\n","src = list(files.upload().values())[0]\n","#open('Treatment.py','wb').write(src)\n","from Treatment import diseaseDetail\n","uploaded = files.upload()\n","# ignore warnings generated due to usage of old version of tensorflow\n","warnings.simplefilter(\"ignore\")\n","from deep_translator import GoogleTranslator"]},{"cell_type":"markdown","metadata":{"id":"t1sbUx8C22zG"},"source":["**Disease Symptom dataset** was created in a separate python program.\n","\n","**Dataset scrapping** was done using **NHP website** and **wikipedia data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txTu6XeVgGAK"},"outputs":[],"source":["# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\n","# Scrapping and creation of dataset csv is done in a separate program\n","df=pd.read_excel(\"/content/drive/My Drive/MAGISTER ILMU KOMPUTER 2021/Group Stanford/Disease-Detection-based-on-Symptoms-master/Dataset/dis_sym_dataset_norm2.xlsx\") # Individual Disease\n","documentname_list=list(df['label_dis'])\n","df=df.iloc[:,1:]\n","columns_name=list(df.columns)\n","documentname_list=list(documentname_list)\n","\n","N=len(df)\n","M=len(columns_name) \n","\n","# All symptoms IDF\n","idf={}\n","for col in columns_name:\n","  temp=np.count_nonzero(df[col])\n","  idf[col]=np.log(N/temp)\n","\n","# All disease,symptom TF\n","tf={}\n","for i in range(N):\n","  for col in columns_name:\n","    key=(documentname_list[i],col)\n","    tf[key]=df.loc[i,col]\n","\n","# All disease,symptom TF.IDF\n","tf_idf={}\n","for i in range(N):\n","  for col in columns_name:\n","    key=(documentname_list[i],col) #key=(penyakit, gejala)\n","    tf_idf[key]=float(idf[col])*float(tf[key])\n","\n","# vector of TF.IDF\n","D = np.zeros((N, M),dtype='float32')\n","for i in tf_idf:\n","    sym = columns_name.index(i[1])\n","    dis=documentname_list.index(i[0])\n","    D[dis][sym] = tf_idf[i]\n","\n","def load(filename):\t\n","\twith open(filename) as data_file:\n","\t\tdata = json.load(data_file)\t\n","  \n","\treturn data\n","  \n","# Data Synonims\n","mydict = load('dict.json')\n","\n","def synonyms(word):\n","\tif word in mydict.keys():\n","\t\treturn set(mydict[word]['sinonim'])\n","\telse:\n","\t\treturn set([])\n","\n","# function for cosine dot product\n","def cosine_dot(a, b):\n","    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:\n","        return 0\n","    else:\n","        temp = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n","        return temp\n","\n","# convert data to lower case\n","def convert_tolowercase(data):\n","    return data.lower()\n","\n","# tokenizing using regextokenizer\n","def regextokenizer_func(data):\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    data = tokenizer.tokenize(data)\n","    return data\n","\n","# function to generate query vector for tf_idf\n","def gen_vector(tokens):\n","    Q = np.zeros(M)\n","    counter = Counter(tokens)\n","    query_weights = {}\n","    for token in np.unique(tokens):\n","        tf = counter[token]\n","        try:\n","          idf_temp=idf[token]\n","        except:\n","          pass\n","        try:\n","            ind = columns_name.index(token)\n","            Q[ind] = tf*idf_temp\n","        except:\n","            pass\n","    return Q\n","\n","# function to calculate tf_idf_score\n","def tf_idf_score(k, query):\n","    query_weights = {}\n","    for key in tf_idf:\n","        if key[1] in query:\n","            try:\n","                query_weights[key[0]] += tf_idf[key]\n","            except:\n","                query_weights[key[0]] = tf_idf[key]\n","    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n","  \n","    l = []\n","    for i in query_weights[:k]:\n","        l.append(i)\n","    return l\n","\n","# function to calculte Cosine Similarity \n","def cosine_similarity(k, query):\n","    d_cosines = []\n","    query_vector = gen_vector(query)\n","    for d in D:\n","        d_cosines.append(cosine_dot(query_vector, d))\n","    out = np.array(d_cosines).argsort()[-k:][::-1]\n","  \n","    final_display_disease={}\n","    for lt in set(out):\n","      final_display_disease[lt] = float(d_cosines[lt])\n","    return final_display_disease"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pyd1WbBq5Ngh"},"outputs":[],"source":["# returns the list of synonyms of the input word from thesaurus.com (https://www.thesaurus.com/) and wordnet (https://www.nltk.org/howto/wordnet.html)\n","def synonyms0(term):\n","    synonyms = []\n","    #response = requests.get('https://www.thesaurus.com/browse/{}'.format(term))\n","    #response = requests.get('https://bahasa.cs.ui.ac.id/iwn/wordnet.php'.format(term))\n","    soup = BeautifulSoup(response.content,  \"html.parser\")\n","    try:\n","        container=soup.find('section', {'class': 'MainContentContainer'}) \n","        row=container.find('div',{'class':'css-191l5o0-ClassicContentCard'})\n","        row = row.find_all('li')\n","        for x in row:\n","            synonyms.append(x.get_text())\n","    except:\n","        None\n","    for syn in wordnet.synsets(term):\n","        synonyms+=syn.lemma_names()\n","    return set(synonyms)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXPUlgHi63Zu"},"outputs":[],"source":["# instantiate objects of libraries\n","splitter = RegexpTokenizer(r'\\w+')\n","\n","#stopwords\n","stop_words = stopwords.words('english')\n","\n","#stemming Sastrawi\n","factory = StemmerFactory()\n","lemmatizer = factory.create_stemmer()"]},{"cell_type":"markdown","metadata":{"id":"DGtPYRLhey0y"},"source":["**Disease Symptom dataset** was created in a separate python program.\n","\n","**Dataset scrapping** was done using **NHP website** and **wikipedia data**"]},{"cell_type":"markdown","metadata":{"id":"rZTXyRhNgN_O"},"source":["Disease Combination dataset contains the combinations for each of the disease present in dataset as practically it is often observed that it is not necessary for a person to have a disease when all the symptoms are faced by the patient or the user.\n","\n","*To tackle this problem, combinations are made with the symptoms for each disease.*\n","\n"," **This increases the size of the data exponentially and helps the model to predict the disease with much better accuracy.**"]},{"cell_type":"markdown","metadata":{"id":"h1LSI08aiDTn"},"source":["*df_comb -> Dataframe consisting of dataset generated by combining symptoms for each disease.*\n","\n","*df_norm -> Dataframe consisting of dataset which contains a single row for each diseases with all the symptoms for that corresponding disease.*\n","\n","**Dataset contains 261 diseases and their symptoms**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpK73qQx5NmJ"},"outputs":[],"source":["# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\n","# Scrapping and creation of dataset csv is done in a separate program\n","df_comb = pd.read_excel(\"/content/drive/My Drive/MAGISTER ILMU KOMPUTER 2021/Group Stanford/Disease-Detection-based-on-Symptoms-master/Dataset/dis_sym_dataset_comb2.xlsx\") # Disease combination\n","df_norm = pd.read_excel(\"/content/drive/My Drive/MAGISTER ILMU KOMPUTER 2021/Group Stanford/Disease-Detection-based-on-Symptoms-master/Dataset/dis_sym_dataset_norm2.xlsx\") # Individual Disease\n","Y = df_norm.iloc[:, 0:1]\n","X = df_norm.iloc[:, 1:]\n","# List of symptoms\n","dataset_symptoms = list(X.columns)\n","diseases = list(set(Y['label_dis']))\n","diseases.sort()"]},{"cell_type":"code","source":["dataICD = pd.read_excel(\"/content/drive/My Drive/MAGISTER ILMU KOMPUTER 2021/Group Stanford/Disease-Detection-based-on-Symptoms-master/Dataset/icd10.xlsx\")\n","icd = dataICD.to_dict(orient = 'list')\n"],"metadata":{"id":"QZMncM2-G8tV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yd8K2QeH5NsL","executionInfo":{"status":"ok","timestamp":1668954960682,"user_tz":-420,"elapsed":2534,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"06e5ed1a-0d18-4777-d7a0-167efb9fad35"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Masukan gejala dipisahkan dengan koma(,):\n","panas 3 hr, batuk pilek, mata blobok, telinga kiri sakit, mata merah sdh 1 mggu, mata kiri bengkak, nyeri, gatal sejak tadi malam, pusing, mata merah, 2hr, MATA MERAH\n"]}],"source":["# Taking symptoms from user as input\n","# Preprocessing the input symtoms \n","user_symptoms = str(input(\"\\nMasukan gejala dipisahkan dengan koma(,):\\n\")).lower().split(',')\n","processed_user_symptoms=[]\n","for sym in user_symptoms:\n","    sym=sym.strip()\n","    sym=sym.replace('-',' ')\n","    sym=sym.replace(\"'\",'')\n","    sym = ' '.join([lemmatizer.stem(word) for word in splitter.tokenize(sym)])\n","    processed_user_symptoms.append(sym)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THRf6-Wa5Nxu","executionInfo":{"status":"ok","timestamp":1668954962940,"user_tz":-420,"elapsed":360,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1978e228-fd2b-4002-852e-2ca48aa5a371"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pengembangan query dari gejala yang dimasukkan\n","['genting a ki erotis panas 3 hr seronok bergolak meriang kering sensual gerah bahang seksi hot  kritis demam menggairahkan a beringsang tegang n kemarau gawat kolor a berbahaya menggiurkan memberahikan merangsang n dedar', 'batuk influenza batuk darah selesma radang selaput lendir batuk pilek batu berdahak', 'preman punat bromocorah indra penglihat ain sumber bajul lubang bangsat perisau gali residivis pusat pencoleng bajingan fokus mata blobok jaharu benih bandit penjahat alat penglihat cecunguk pokok durjana punca tunas netra titik berat arah', 'ambruk kuping linu nyeri a pedih kidal mulas lara telinga kiri sakit ngilu perih sakit gering', 'preman punat bromocorah mata merah sdh 1 mggu indra penglihat ain sumber bajul lubang biram bangsat perisau sirah bangkang gali residivis pusat berma pencoleng bajingan fokus abang jaharu benih bandit penjahat ahmar alat penglihat cecunguk pokok durjana punca tunas netra titik berat arah', 'punat bengep ain buncit bakup balut bonggal barah menyonyor bedan alat penglihat titik berat sembam bentol bundas bromocorah kidal bengul bonjol bintil bon-col bangsat jendul dengkel benggil beliut sembap babak belur pusat fokus penjahat pokok bajul bintul lasa indra penglihat tembam lubang benjol gali residivis pencoleng basal jaharu gembung cecunguk bengkil jendol arah preman tonjol bokong sumber mata kiri bengkak perisau benjut besar benggol melendung bajingan bisul melembung benih bandit durjana punca tunas netra', 'sakit nyeri', 'menggelinyau semula menggerenyam senja petang semenjak mulai sedianya dari renyem gatal sejak tadi malam gasang galak jelingah mulanya adv sebetulnya sore giang mengerinyau merenyam per lilin lebah lilin batik', 'puyeng mangut pitam ganar mumet a bingung tersuntuk sakit kepala mabuk nanar jangar pakau teler pusing lengar kliyengan  a berpendar mamang teringainga gayang putar merayang pening', 'preman punat bromocorah indra penglihat ain sumber bajul lubang biram bangsat perisau sirah bangkang mata merah gali residivis pusat berma pencoleng bajingan fokus abang jaharu benih bandit penjahat ahmar alat penglihat cecunguk pokok durjana punca tunas netra titik berat arah', '2hr', 'preman punat bromocorah indra penglihat ain sumber bajul lubang biram bangsat perisau sirah bangkang mata merah gali residivis pusat berma pencoleng bajingan fokus abang jaharu benih bandit penjahat ahmar alat penglihat cecunguk pokok durjana punca tunas netra titik berat arah']\n"]}],"source":["# Taking each user symptom and finding all its synonyms and appending it to the pre-processed symptom string\n","user_symptoms = []\n","for user_sym in processed_user_symptoms:\n","    user_sym = user_sym.split()\n","    str_sym = set()\n","    for comb in range(1, len(user_sym)+1):\n","        for subset in combinations(user_sym, comb):\n","            subset=' '.join(subset)\n","            subset = synonyms(subset) \n","            str_sym.update(subset)\n","    str_sym.add(' '.join(user_sym))\n","    user_symptoms.append(' '.join(str_sym).replace('_',' '))\n","# query expansion performed by joining synonyms found for each symptoms initially entered\n","print(\"Pengembangan query dari gejala yang dimasukkan\")\n","print(user_symptoms)"]},{"cell_type":"markdown","metadata":{"id":"7sPyVlJIjdv2"},"source":["The below procedure is performed in order to show the symptom synonmys found for the symptoms entered by the user.\n","\n","The symptom synonyms and user symptoms are matched with the symptoms present in dataset. Only the symptoms which matches the symptoms present in dataset are shown back to the user. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFUYSnLU5Nu-"},"outputs":[],"source":["# Loop over all the symptoms in dataset and check its similarity score to the synonym string of the user-input \n","# symptoms. If similarity>0.5, add the symptom to the final list\n","found_symptoms = set()\n","for idx, data_sym in enumerate(dataset_symptoms):\n","    data_sym_split=data_sym.split()\n","    for user_sym in user_symptoms:\n","        count=0\n","        for symp in data_sym_split:\n","            if symp in user_sym.split():\n","                count+=1\n","        if count/len(data_sym_split)>0.5: # this is jaccard similarity\n","            found_symptoms.add(data_sym)\n","found_symptoms = list(found_symptoms)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBV1qPFv5NpY","executionInfo":{"status":"error","timestamp":1668954972268,"user_tz":-420,"elapsed":2408,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}},"colab":{"base_uri":"https://localhost:8080/","height":791},"outputId":"a6809dbc-d41c-4236-96a9-06fc3fcd5e9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gejala pencocokan teratas dari pencarian Anda!\n","0 : sakit telinga\n","1 : batuk lendir berdarah\n","2 : batuk darah\n","3 : merah\n","4 : demam\n","5 : mata merah terbakar\n","6 : nyeri\n","7 : batuk lendir\n","8 : pusing\n","9 : batuk\n","10 : bengkak\n","11 : batuk termasuk batuk darah\n","12 : sakit\n","13 : sakit kepala\n","14 : gatal\n","15 : batuk berdahak\n","16 : mata merah\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-572806ac7a9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Show the related symptoms found in the dataset and ask user to select among them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mselect_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSilakan pilih gejala yang relevan. Masukkan indeks (dipisahkan dengan spasi):\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Find other relevant symptoms from the dataset based on user symptoms based on the highest co-occurance with the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["# Print all found symptoms\n","print(\"Gejala pencocokan teratas dari pencarian Anda!\")\n","for idx, symp in enumerate(found_symptoms):\n","    print(idx,\":\",symp)\n","\n","# Show the related symptoms found in the dataset and ask user to select among them\n","select_list = input(\"\\nSilakan pilih gejala yang relevan. Masukkan indeks (dipisahkan dengan spasi):\\n\").split()\n","\n","# Find other relevant symptoms from the dataset based on user symptoms based on the highest co-occurance with the\n","# ones that is input by the user\n","dis_list = set()\n","final_symp = [] \n","counter_list = []\n","for idx in select_list:\n","    symp=found_symptoms[int(idx)]\n","    final_symp.append(symp)\n","    dis_list.update(set(df_norm[df_norm[symp]==1]['label_dis']))\n","   \n","for dis in dis_list:\n","    row = df_norm.loc[df_norm['label_dis'] == dis].values.tolist()\n","    row[0].pop(0)\n","    for idx,val in enumerate(row[0]):\n","        if val!=0 and dataset_symptoms[idx] not in final_symp:\n","            counter_list.append(dataset_symptoms[idx])\n","\n","# Symptoms that co-occur with the ones selected by user              \n","dict_symp = dict(Counter(counter_list))\n","dict_symp_tup = sorted(dict_symp.items(), key=operator.itemgetter(1),reverse=True)   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgvzIn7Q5NjV","executionInfo":{"status":"ok","timestamp":1668954583620,"user_tz":-420,"elapsed":3380,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"991fab60-d6c6-4a0e-c5ca-6e5cb2b29d14"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Gejala umum yang terjadi bersamaan:\n","0 : demam\n","1 : sakit kepala\n","2 : diare\n","3 : nyeri testis\n","4 : kebingungan\n","Apakah Anda memiliki gejala-gejala tersebut? Jika Ya, masukkan indeks (dipisahkan dengan spasi), 'tidak' untuk berhenti, '-1' untuk melewati:\n","tidak\n"]}],"source":["# Iteratively, suggest top co-occuring symptoms to the user and ask to select the ones applicable \n","found_symptoms=[]\n","count=0\n","for tup in dict_symp_tup:\n","    count+=1\n","    found_symptoms.append(tup[0])\n","    if count%5==0 or count==len(dict_symp_tup):\n","        print(\"\\nGejala umum yang terjadi bersamaan:\")\n","        for idx,ele in enumerate(found_symptoms):\n","            print(idx,\":\",ele)\n","        select_list = input(\"Apakah Anda memiliki gejala-gejala tersebut? Jika Ya, masukkan indeks (dipisahkan dengan spasi), 'tidak' untuk berhenti, '-1' untuk melewati:\\n\").lower().split();\n","        if select_list[0]=='tidak':\n","            break\n","        if select_list[0]=='-1':\n","            found_symptoms = [] \n","            continue\n","        for idx in select_list:\n","            final_symp.append(found_symptoms[int(idx)])\n","        found_symptoms = []    "]},{"cell_type":"markdown","metadata":{"id":"nI5taHc8pfY3"},"source":["Final Symptom list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYPReN9D5Nd_","executionInfo":{"status":"ok","timestamp":1668954583621,"user_tz":-420,"elapsed":11,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"856b2467-1840-4919-d5ce-2dcd92975b1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Daftar akhir dari gejala yang diberikan untuk prediksi adalah : \n","biasanya tidak ada\n","muntah\n","pusing\n","nyeri\n","mual\n","tidak bisa bergerak\n","sakit perut\n","berkepanjangan\n"]}],"source":["#Calculating TF-IDF and Cosine Similarity using matched symptoms\n","k = 10\n","\n","print(\"Daftar akhir dari gejala yang diberikan untuk prediksi adalah : \")\n","for val in final_symp:\n","    print(val)"]},{"cell_type":"markdown","metadata":{"id":"8_A-6Dl5qHlv"},"source":["# **Showing the list of top k diseases to the user with their prediction probabilities.**\n","\n","# **For getting information about the suggested treatments, user can enter the corresponding index to know more details.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWUsVkF3t6jk","executionInfo":{"status":"ok","timestamp":1668954590283,"user_tz":-420,"elapsed":6671,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"607d3acd-5965-4841-a606-985497e9daff"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Hasil 10 teratas prediksi penyakit dengan pencocokan TF_IDF  :\n","\n","0. Penyakit : Myocardial Infarction (Heart Attack) \t Score : 8.73\n","1. Penyakit : Dehydration \t Score : 7.34\n","2. Penyakit : Heat-Related Illnesses and Heat waves \t Score : 7.34\n","3. Penyakit : Carbon monoxide poisoning \t Score : 6.85\n","4. Penyakit : Anthrax \t Score : 5.84\n","5. Penyakit : Calculi \t Score : 5.84\n","6. Penyakit : Hepatitis A \t Score : 5.84\n","7. Penyakit : Hepatitis C \t Score : 5.56\n","8. Penyakit : Mad cow disease \t Score : 5.56\n","9. Penyakit : Post Menopausal Bleeding \t Score : 5.56\n","\n","Perlu lebih banyak detail tentang penyakitnya? Masukkan indeks penyakit atau '-1' untuk menghentikan:\n","0\n","\n","Infark Miokard (Serangan Jantung)\n","Nama lain - Infark miokard akut (AMI), serangan jantung\n","Spesialisasi - Kardiologi, pengobatan darurat\n","Gejala - Nyeri dada, sesak napas, mual, merasa ingin pingsan, keringat dingin, merasa lelah, lengan, leher, punggung, rahang, atau sakit perut\n","Komplikasi - Gagal jantung, detak jantung tidak teratur, syok kardiogenik, henti jantung\n","Penyebab - Biasanya penyakit arteri koroner\n","Faktor risiko - Tekanan darah tinggi, merokok, diabetes, kurang olahraga, obesitas, kolesterol darah tinggi\n","Metode diagnostik - Elektrokardiogram (EKG), tes darah, angiografi koroner\n","Pengobatan - Intervensi koroner perkutan, trombolisis\n","Obat - Aspirin, nitrogliserin, heparin\n","Prognosis - STEMI 10% risiko kematian (negara maju)\n","Frekuensi - 159 juta (2015)\n","\n","Kode ICD 10 untuk  Myocardial Infarction (Heart Attack)  :  I25\n"]}],"source":["topk1=tf_idf_score(k,final_symp)\n","topk2=cosine_similarity(k,final_symp)\n","# Show top 10 highly probable disease to the user.\n","print(f\"\\nHasil {k} teratas prediksi penyakit dengan pencocokan TF_IDF  :\\n\")\n","i = 0\n","topk1_index_mapping = {}\n","for key, score in topk1:\n","  print(f\"{i}. Penyakit : {key} \\t Score : {round(score, 2)}\")\n","  topk1_index_mapping[i] = key\n","  i += 1\n","\n","select = input(\"\\nPerlu lebih banyak detail tentang penyakitnya? Masukkan indeks penyakit atau '-1' untuk menghentikan:\\n\")\n","if select!='-1':\n","    dis=topk1_index_mapping[int(select)]\n","    print()\n","    print(GoogleTranslator(source='auto', target='id').translate(diseaseDetail(dis)))\n","    print()\n","    print('Kode ICD 10 untuk ', dis, ' : ', icd[dis][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHiGmHhqdnMs","executionInfo":{"status":"ok","timestamp":1668954596102,"user_tz":-420,"elapsed":5825,"user":{"displayName":"Uffi Nadzima","userId":"04559101785503367249"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f6195e3d-5cef-4b3c-a107-8324cd5cddd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hasil 10 teratas prediksi penyakit dengan pencocokan Cosine Similarity :\n"," \n","0. Penyakit : Hepatitis C \t Score : 0.4\n","1. Penyakit : Mad cow disease \t Score : 0.24\n","2. Penyakit : Carbon monoxide poisoning \t Score : 0.23\n","3. Penyakit : Dehydration \t Score : 0.22\n","4. Penyakit : Post Menopausal Bleeding \t Score : 0.21\n","5. Penyakit : Campylobacter infection \t Score : 0.19\n","6. Penyakit : Myocardial Infarction (Heart Attack) \t Score : 0.19\n","7. Penyakit : Sickle-cell anemia \t Score : 0.19\n","8. Penyakit : Heat-Related Illnesses and Heat waves \t Score : 0.18\n","9. Penyakit : Malaria \t Score : 0.15\n","\n","Perlu lebih banyak detail tentang penyakitnya? Masukkan indeks penyakit atau '-1' untuk menghentikan dan menutup sistem:\n","0\n","\n","Hepatitis C\n","Spesialisasi - Gastroenterologi, Penyakit Menular\n","Gejala - Biasanya tidak ada\n","Komplikasi - Gagal hati, kanker hati, varises esofagus dan lambung\n","Durasi - Jangka panjang (80%)\n","Penyebab - Virus hepatitis C biasanya menyebar melalui kontak darah-ke-darah\n","Metode diagnostik - Tes darah untuk antibodi atau RNA virus\n","Pencegahan - Jarum steril, pengujian darah yang disumbangkan\n","Pengobatan - Pengobatan, transplantasi hati\n","Obat - Antivirus (sofosbuvir, simeprevir, lainnya)\n","Frekuensi - 58 juta (2019)\n","Kematian - 290.000 (2019)\n","\n","Kode ICD 10 untuk  Hepatitis C  :  B18\n"]}],"source":["# display top k diseases predicted with cosine probablity\n","print(f\"Hasil {k} teratas prediksi penyakit dengan pencocokan Cosine Similarity :\\n \")\n","topk2_sorted = dict(sorted(topk2.items(), key=lambda kv: kv[1], reverse=True))\n","j = 0\n","topk2_index_mapping = {}\n","for key in topk2_sorted:\n","  print(f\"{j}. Penyakit : {diseases[key]} \\t Score : {round(topk2_sorted[key], 2)}\")\n","  topk2_index_mapping[j] = diseases[key]\n","  j += 1\n","\n","    \n","select = input(\"\\nPerlu lebih banyak detail tentang penyakitnya? Masukkan indeks penyakit atau '-1' untuk menghentikan dan menutup sistem:\\n\")\n","if select!='-1':\n","    dis=topk2_index_mapping[int(select)]\n","    print()\n","    print(GoogleTranslator(source='auto', target='id').translate(diseaseDetail(dis)))\n","    print()\n","    print('Kode ICD 10 untuk ', dis, ' : ', icd[dis][0])"]},{"cell_type":"code","source":[],"metadata":{"id":"3dluJnN6MQHB"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}